{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, data):\n",
    "    interpreter = tf.lite.Interpreter(model_path = model)\n",
    "    # get input and output tensors.\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    interpreter.resize_tensor_input(input_details[0]['index'], data.shape)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    iter=3\n",
    "    result = []\n",
    "    for i in range(iter):\n",
    "        output_data = interpreter.get_tensor(output_details[i]['index'])\n",
    "        result.append(output_data)\n",
    "\n",
    "    # output_data\n",
    "    return result\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "# 定义预处理函数，将像素值约束到[-1, 1]范围\n",
    "def preprocess_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),           # 将图片转换为PyTorch的Tensor数据类型\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),   # 标准化到[-1, 1]\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)  # 添加batch维度，因为模型输入一般为(batch_size, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y   \n",
    "\n",
    "def process(input, mask, anchors):\n",
    "    anchors = [anchors[i] for i in mask]\n",
    "    grid_h, grid_w = map(int, input.shape[0:2])\n",
    "\n",
    "    box_confidence = sigmoid(input[..., 4])\n",
    "    box_confidence = np.expand_dims(box_confidence, axis=-1)\n",
    "\n",
    "    box_class_probs = sigmoid(input[..., 5:])\n",
    "\n",
    "    box_xy = sigmoid(input[..., :2]) * 2 - 0.5\n",
    "\n",
    "    col = np.tile(np.arange(0, grid_w), grid_w).reshape(-1, grid_w)\n",
    "    row = np.tile(np.arange(0, grid_h).reshape(-1, 1), grid_h)\n",
    "    col = col.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)\n",
    "    row = row.reshape(grid_h, grid_w, 1, 1).repeat(3, axis=-2)\n",
    "    grid = np.concatenate((col, row), axis=-1)\n",
    "    box_xy += grid\n",
    "    box_xy *= int(IMG_SIZE / grid_h)\n",
    "\n",
    "    box_wh = pow(sigmoid(input[..., 2:4]) * 2, 2)\n",
    "    box_wh = box_wh * anchors\n",
    "\n",
    "    box = np.concatenate((box_xy, box_wh), axis=-1)\n",
    "\n",
    "    return box, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def filter_boxes(boxes, box_confidences, box_class_probs):\n",
    "    \"\"\"Filter boxes with box threshold. It's a bit different with origin yolov5 post process!\n",
    "    # Arguments\n",
    "        boxes: ndarray, boxes of objects.\n",
    "        box_confidences: ndarray, confidences of objects.\n",
    "        box_class_probs: ndarray, class_probs of objects.\n",
    "    # Returns\n",
    "        boxes: ndarray, filtered boxes.\n",
    "        classes: ndarray, classes for boxes.\n",
    "        scores: ndarray, scores for boxes.\n",
    "    \"\"\"\n",
    "    global BOX_THRESH\n",
    "    box_classes = np.argmax(box_class_probs, axis=-1)\n",
    "    box_class_scores = np.max(box_class_probs, axis=-1)\n",
    "    pos = np.where(box_confidences[..., 0] >= BOX_THRESH)\n",
    "    \n",
    "    boxes = boxes[pos]\n",
    "    classes = box_classes[pos]\n",
    "    scores = box_class_scores[pos]\n",
    "    \n",
    "    return boxes, classes, scores\n",
    "\n",
    "def nms_boxes(boxes, scores):\n",
    "    \"\"\"Suppress non-maximal boxes.\n",
    "\n",
    "    # Arguments\n",
    "        boxes: ndarray, boxes of objects.\n",
    "        scores: ndarray, scores of objects.\n",
    "\n",
    "    # Returns\n",
    "        keep: ndarray, index of effective boxes.\n",
    "    \"\"\"   \n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    w = boxes[:, 2] - boxes[:, 0]\n",
    "    h = boxes[:, 3] - boxes[:, 1]\n",
    "    \n",
    "    areas = w*h  # 预测框面积；\n",
    "    order = scores.argsort()[::-1]  # 对scores排序，找到scores最大的; argsort(),返回的是元素值从小到大排序后的索引值的数组，[::-1]则表示从大到小排序；\n",
    "    \n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        xx1 = np.maximum(x[i], x[order[1:]])  # 找出最大的x1, np.maximum(a, b) --> a与b逐位比较取其大者\n",
    "        yy1 = np.maximum(y[i], y[order[1:]])  # 找出最大的y1\n",
    "        xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])\n",
    "        yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])\n",
    "        \n",
    "        w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)\n",
    "        h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)\n",
    "        inter = w1 * h1  # 计算交集面积\n",
    "        \n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        inds = np.where(ovr <= NMS_THRESH)[0]  # 返回交并比小于阈值的索引序列，并取第0个索引。\n",
    "        order = order[inds + 1]\n",
    "    \n",
    "    keep = np.array(keep)\n",
    "    return keep\n",
    "        \n",
    "\n",
    "def yolov5_post_process(input_data):\n",
    "    boxes, classes, scores = [], [], []\n",
    "    for input, mask in zip(input_data, masks):\n",
    "        b, c, s = process(input, mask, anchors)\n",
    "        b, c, s = filter_boxes(b, c, s)  # filter boxes with box threshold\n",
    "        boxes.append(b)\n",
    "        classes.append(c)\n",
    "        scores.append(s)\n",
    "    \n",
    "    boxes = np.concatenate(boxes)\n",
    "    boxes = xywh2xyxy(boxes)\n",
    "    classes = np.concatenate(classes)\n",
    "    scores = np.concatenate(scores)\n",
    "    \n",
    "    nboxes, nclasses, nscores = [], [], []\n",
    "    for c in set(classes):\n",
    "        inds = np.where(classes == c)\n",
    "        b = boxes[inds]\n",
    "        c = classes[inds]\n",
    "        s = scores[inds]\n",
    "        \n",
    "        keep = nms_boxes(b, s)\n",
    "        \n",
    "        nboxes.append(b[keep])\n",
    "        nclasses.append(c[keep])\n",
    "        nscores.append(s[keep])\n",
    "        \n",
    "    if not nclasses and not nscores:\n",
    "        return None, None, None\n",
    "    \n",
    "    boxes = np.concatenate(nboxes)\n",
    "    classes = np.concatenate(nclasses)\n",
    "    scores = np.concatenate(nscores)\n",
    "    \n",
    "    return boxes, classes, scores   \n",
    "\n",
    "def draw(image, boxes, scores, classes):\n",
    "    \"\"\"draw the boxes on the image\n",
    "\n",
    "    Args:\n",
    "        image (_type_): original image\n",
    "        boxes (ndarray): boxes of objects\n",
    "        scores (ndarray): scores of objects\n",
    "        classes (ndarray): classes of objects\n",
    "    \"\"\"\n",
    "    for box, score, cl in zip(boxes, scores, classes):\n",
    "        top, left, right, bottom = box\n",
    "        print('class:{}, score:{}'.format(CLASSES[cl], score))\n",
    "        print('box coordinate left, top, right, down: [{}, {}, {}, {}]'.format(top, left, right, bottom))\n",
    "        top = int(top)\n",
    "        left = int(left)\n",
    "        right = int(right)\n",
    "        bottom = int(bottom)\n",
    "        \n",
    "        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)\n",
    "        cv2.putText(image, '{0}{1:.2f}'.format(CLASSES[cl], score), (top, left-6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: 3\n",
      "(1, 40, 3, 7, 40) (1, 10, 3, 7, 10) (1, 20, 3, 7, 20)\n",
      "(3, 40, 7, 40)\n",
      "(40, 40, 3, 7) (10, 10, 3, 7) (20, 20, 3, 7)\n",
      "(359, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11158/3141466039.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 1 (CV_8S)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m boxes, classes, scores \u001b[38;5;241m=\u001b[39m yolov5_post_process(input_data)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(boxes))\n\u001b[0;32m---> 56\u001b[0m img_1 \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2BGR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     draw(img_1, boxes, scores, classes)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 1 (CV_8S)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BOX_THRESH=0.2\n",
    "    NMS_THRESH=0.6\n",
    "    IMG_SIZE=320\n",
    "    model = \"./phone_int8_0630.tflite\"\n",
    "    img_path = \"./20230703/0_frame_000000000.bmp\"\n",
    "    CLASSES=(\"person\", \"phone\")\n",
    "    IMG_PATH = \"./20230703/0_frame_000000000.bmp\"\n",
    "    SAVE_PATH = \"./result1.png\"\n",
    "        \n",
    "    # 手机检测    \n",
    "    masks = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    anchors = [[17, 21], [23, 36], [32, 57], \n",
    "               [40, 98], [54, 136], [68, 189], \n",
    "               [80, 122], [93, 249], [114, 167]]\n",
    "    \n",
    "    \n",
    "    img = cv.imread(img_path) \n",
    "    # img = np.array([img])-128.0\n",
    "    img = np.array([img])/128-1.0\n",
    "    img = np.array(img, dtype='int8')\n",
    "    # img = preprocess_image(img)\n",
    "    # img = np.transpose(img, (0, 2, 3, 1))\n",
    "    # img = img/255-0.5\n",
    "    output = run_inference(model, img)\n",
    "    print(\"output shape:\",len(output))\n",
    "    print(np.shape(output[0]), np.shape(output[1]), np.shape(output[2]))   \n",
    "    \n",
    "    # post process\n",
    "    input0_data = output[0]\n",
    "    input1_data = output[1]\n",
    "    input2_data = output[2] \n",
    "    \n",
    "    \"\"\"\n",
    "        output tensor dim: anchors * (5+num_cls) * maps_h * mapx_w\n",
    "        while anchors = 3, 5 means [offset_x, offset_y, offset_w, offset_h, object_confidence]\n",
    "    \"\"\"  \n",
    "    input0_data = input0_data.reshape([3,-1] + list(input0_data.shape[-2:])) \n",
    "    input1_data = input1_data.reshape([3,-1] + list(input1_data.shape[-2:]))\n",
    "    input2_data = input2_data.reshape([3,-1] + list(input2_data.shape[-2:]))\n",
    "\n",
    "    print(np.shape(input0_data))    \n",
    "    input_data = list()\n",
    "    input_data.append(np.transpose(input0_data, (1, 3, 0, 2)))\n",
    "    input_data.append(np.transpose(input1_data, (1, 3, 0, 2)))\n",
    "    input_data.append(np.transpose(input2_data, (1, 3, 0, 2)))\n",
    "    \n",
    "    \n",
    "    print(np.shape(input_data[0]), np.shape(input_data[1]), np.shape(input_data[2]))    \n",
    "    boxes, classes, scores = yolov5_post_process(input_data)\n",
    "    print(np.shape(boxes))\n",
    "    \n",
    "    img_1 = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    if boxes is not None:\n",
    "        draw(img_1, boxes, scores, classes)\n",
    "        \n",
    "    cv2.imwrite(SAVE_PATH, img_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('AI_env_TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4cc0dc986b19ae6fe037aa86599ba1d74ac16f91af58e4b46471c2c1b461759"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
